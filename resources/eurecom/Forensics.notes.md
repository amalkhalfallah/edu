# Cybercrime and computer forensics

Notes by Nino Filiu; based on the Forensics course by David Balzarotti.

| Slides file | completion status |
| --- | --- |
| [intro](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/malware_intro.pdf) | done |
| [static analysis A](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/static_analysis_a_2018-03-22_10-54-54_196.pdf) | done |
| [static analysis B](https://my.eurecom.fr/upload/docs/application/pdf/2018-03/static_analysis_b.pdf) | done |
| [reverse engineering tools](https://my.eurecom.fr/upload/docs/application/pdf/2018-04/reveng_tools.pdf) | ignored |
| [dynamic analysis A](https://my.eurecom.fr/upload/docs/application/pdf/2018-04/malware_analysis_sandboxes.pdf) | done |
| [dynamic analysis B](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/dynamic_analysis_part_b.pdf) | done |
| [computer forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/forensics_intro_wide.pdf) | done |
| [memory forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/memory_forensics.pdf) | done |
| [network forensics A](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/network_forensics_first_half.pdf) | done |
| [network forensics B](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/network_forensics.pdf) | done |
| [disk and filesystem forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/disk_forensics.pdf) | done |
| [forensics techniques](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/basic_techniques.pdf) | done |
| [logical reasonning A](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/logical_reasoning_1.pdf) | ignored |
| [logical reasonning B](https://my.eurecom.fr/upload/docs/application/pdf/2018-05/logical_reasoning_2.pdf) | ignored |
| [OS and app forensics](https://my.eurecom.fr/upload/docs/application/pdf/2018-06/os_apps_forensics.pdf) | to do |

## Binary and malware analysis

### Glossary

A **malware** is a piece of software intentionally designed with malicious functionalities. SQL injections and the likes are not malware because they are not software. Vulnerable applications are not malwares because the faults are not intentional. Malicious = against the user's will. These 3 concepts are blurry so the frontier between what's a malware and what's not is blurry too.

There are tons of types of malware: **viruses, worms, trojan, ransomware, botnets, adware, cryptominers**... The course is not aiming at covering them all, rather studying some of them when they can be used as examples.

**Binary** can denote program binary and data binary. Program binary is usually generated by a compiler... but not always! The world of well-formed binaries that use functions, stacks, heaps and the likes is theoretically a grain of sand in the Sahara of possible executables. Binaries mostly come in these forms though: **executable files, libraries, firmware images, and process dumps**.

Malicious binaries are often very **adversarial**: their symbols are stripped of, they are obsfuscated, packed, and full of tricks (anti-debugging, suicide bombs, checks for sandbox...).

### History

**80's: the origins**: first widespread outbreak of a virus. Leonard Adleman coins the term virus. first antivirus company (McAfee). First worm.

**90's: complexity grows**: the term malware is coined. Polymorphic viruses. Virus that damages AV. Melissa. Kernel rootkits.

**00's: the new millenium**: code red. SQL slammer. Spam botnets.

**05's: cybercrime**: Storm (worm+botnet). Zeus banking trojan. Koobface. Microsoft offers $250K who developed Conficker, the first pandemic virus.

**10's: state-sponsored**: Stuxnet sabotages iranian nuclear facilities. Gauss. leak of the NSA ANT catalog. the Mirai botnet is responsible for a large DDoS attack using infected IoT. increase in mobile infection.

### Malware analysis

The goal is to find out: 

* what the malware can do
* what the malware actually does
* which family does it belongs to
* how can it be detected
* how can it be blocked
* how can it be removed

Part of the larger field that is reverse engineering. Reverse engineering has many legitimate applications but it is not legal everywhere in the same form. Due to the large amount of pieces of code to analyze, automated binary analysis is often used: there is however a trade-off between scalability and precision+unbiasement. Several stages to detect a malware:

```
def detect(prog):
    if prog.isKnown:
        # often fail because of polymorphism
        return true
    if prog.matchesAKnownSignature:
        # signatures are often imprecise
        return true
    if staticAnalysis(prog):
        # malware code is highly obfuscated, encrypted, self-modified...
        return true
    if dynamicAnalysis(prog):
        # many expensive parameters
        # limited time
        # can behave badly only at certain times
        # sandbox detection
        return true
    return false
```

So malware detection is hard. Anlasysis are done in an adversial environment so the data is noisy and ML performs badly, plus there is a demand for a zero tolerance on false positive. Complex problems that range from microscopic programming (flipping bits) to macroscopic programming (intelligence from billions of aggregated infos).

This have to be done for every sample, and there are approx 1M samples/day.

Dynamic analysis are more precise because it tells you everything the program can do. However, it achieves a smaller coverage, given that it observes one execution path at a time. Static analysis, on the contrary, are less precise because it is needed to reason about the program behavior without executing it, but it achieves a larger coverage because it can reason about all possible executions at the same time.



## Static analysis

### Black box

Overview:

* known binary: check for the file hash
* similar to something known: check the signatures
* looking for hints: study embedded strings, imported libs, file headers, and symbols

Diving into it requires some knowledge about available tools and additional knowledge about binaries.

**VirusTotal** is a service that can help with file hashes - it knows more than 1B of files. **Yara** is a language to describe byte-level patterns, and a tool to match the patterns against files - kind of a grep on steroids. Command-line or Python-API usable. **strings** is a command-line utility that locates the printable strings of characters in a file. **Windows's dependency-walker and Linux' ldd/lddtree** check for dynamic libraries linking. Analysis is much harder for statically linked ones - their binary is not really differentiable from the program's binary.

Binary file format defines what the file looks like on disk and how it should be loaded in memory. **PE (portable executable)** is used to represent executables in windows, **ELF (executable and linkable format)** is the equivalent in UNIX-like OS.

#### Portable executable format

Contains multiple section of either code or data, each of them having its particular RWX attributes shared by all processes running the executable. Compilers have a standard set of sections but programmers are free to create and name arbitrary ones. Memory addresses are expressed using RVA (relative virtual addresses). A Data Directory array is used to locate other artifacts and data structures inside the PE. Each imported DLL has a structure in the PE, containing the name of the DLL and an array of function pointers known as the Import Address Table (IAT). Structure:

```
MS-DOS
    header
    stub
PE header
    file header
    optional header
sections table
    section 1 header
    ...
    section N header
section 1
...
section N
```

Explanation:

* MS-DOS: small executable that checks for windows
* PE file header: basic infos: 32 or 64 bits, nb of sections...
* PE optional header: optional infos: size of code, entry point address, size of headers...
* common sections (not extensive):
	* .text: default code
	* .data: default RW data section (global variables)
	* .rdata: default R data section
	* .idata: imports table, one entry per imported library.

**HT** is a utility for editing and visualizing the executable sections. **pefile** is a python lib to analyze PE files, thus the name pefile lol. Suspicious PE attributes can be used to flag potentially malicious binaries: weird entry points, high entropy = packers... **pescanner.py** can be used to locate such things, PEStudio too.

#### Executable and linkable format

```
ELF header
    // file type, machine arch, magic number, code entry point...
Program header table
    // type, offset, vaddr and paddr...
Section 1
...
Section N
    // .bss, .data, .rodata, .text...
Section header table
    // for each section, a name, a type, flags, address, offset, and the likes are precised
```

Since the loader and dynamic linker only reason in terms of segments, section information is not required at runtime. One can get rid of this section with `truncate -s $(readelf -h file.elf | grep -F 'Start of section headers' | awk '{print $5}') file.elf.`

The symbol table holds quite precious (on a human point of view) infos: name/value/size/section of each symbol, global/local/weak binding of them... Programs still run with this table removed but all the names of the program functions and global variables are lost. In the case of statically-linked programs, hundreds of nameless library functions are mixed with the program code.

Useful functions:

```
$ readelf <options> <filename>
    - print headers
    - print the symbols
    - print the notes
$ ldd <progname>
$ lddtree <progname>
    - list the shared libs required by the prog
$ truncate -s $(readelf -h file.elf | grep -F 'Start of section headers' | awk '{print $5}') file.elf.
		The loader and linker only reason in terms of segments so section info is not required at runtime.
		This command strips it out.
```

### Assembly 101

Two main families of instruction sets: RISC and CISC. An assembler does the machine code - assembly translation, assembly being the human readable and (almost) instruction-to-instruction equivalent of the machine code. Let's take a closer look at the x86 CISC assembler.

* Each instruction is in the form mnemonic arguments
* 0 to 2 arguments
* constant, registers, or mem addresses arguments
* variable length
* two main syntax: intel (used in this course) and AT&T (default).

Registers can be accessed like so:

```
RAX [ 0:64] bits
EAX [32:64] bits
AH  [32:48] bits
AL  [48:64] bits
```

In x86, instructions fall in several categories:

* data movement: mov, push, pop, lea...
* arithmetic and logic: add, sub, mul, not, and, or...
* control flow: jmp, call, ret...

There are many calling conventions, the one used in C (CDEL) is the standard. Arguments are passed on the sack right to left, returnn value is placed in the EAX, the calling function cleans the stack, and EAX/ECX/EDX are free to use. However, there are many calling conventions: STDCALL→called function cleans the stack, FASTCALL→first params passed in registers...

An ABI (application binary interface) defines the interface between software modules or userspace to kernel communication: calling convention, data types...

### Disassembly

Assembly is deterministic but disassembly is not, ie from a sequence of bytes it is theoretically undecideable to deduce a single assembly code with absolute certainty (surprising, eh?). Main reasons:

* depending from which byte the disassembler starts, different instructions can be deduced
* data and code are mixed and it's not always feasible to know which is which

Two main approaches to disassemble a program: linear sweep = instr per instr, recursive traversal = control flow is followed. Linear sweep is simpler but assumes a well-behaved compiler that packs every instruction together: objdump, gdb, windbg use this approach. In recursive traversal, it's hard to know which branch to disassemble without actually running the program, and that's not even taking into account overlapping functions self-modifying code.

But disassemblers have tricks to counter these difficulties: functions are often detected easily thanks to their prologue (usual sequences of code before a function).

### Decompilation

dissassembly → code. Several limitations:

* requires a perfect disassembly
* compilation is lossy
* complex data structure makes the code hard to read
* compilation = many-to-many operation, theoretically impossible to reverse with certainty
* language and computer-specific
* expensive lol (hexrays = 2600 buck)

Usually follow this pattern:

1. disassembly
2. dataflow analysis: recognize variables, detach them from registers and memory addresses; identify function args
3. control flow analysis:
4. type analysis

### Limits of static analysis

In a nutshell:

* packing
* indirect jump prediction
* obfuscated addresses
* limits of disassembly:
	* overlapping instructions
	* fake conditional jumps
	* return address modification

### Packers

`code ---(compression and/or encryption)--> unpacking stub + data`

The unpacking stub retrieves the original code from the data. The data is often stored in weird PE sections. It also resolves the imports. It then `jump/call/ret` to the OEP (original entry point). Source of a lot of complication for the analysis:

* **Recursive layers**
* **Interleaved packing**: no clear jump tail, but the code is interleaved with the code of the original app
* **Partial unpacking**: unpack only one function or block at a time. Sometimes re-pack it so that the code is never fully unpacked (=statically analisable)
* **Emulation**: translate the instructions into a random bytecode. The stub includes an interpreter

There are many packers but their presence can often be detected by:

* Few functions
* Few entries in the import table
* Small code but requires more space in the memory disk
* Sections with weird names
* Sections with high entropy

Tools like SigBuster and PeiD can be used to identify packers.

The many packers that exist can be grouped into types:

1. single-layer
2. multi-layer, linear transitions
3. cyclic transition, tail isolation
4. interleaved isolation, single frame
5. multi-frame, incremental frame switch
6. shifting decode frames

A few % of packers are of types 5 and above, but less than 30 of types 2 and below. Most packers aare of type 3.

There also exists several types of unpacking techniques:

* **Automatic + static**: very few cases that are not security-oriented
* **Automatic + dynamic**: euristics to detect the OEP. Works well for simple packers
* **Manual + static**: requires a reversing of the stub
* **Manual + dynamic**: use a debugger to manually id the OEP

## Dynamic analysis

Tells you exactly what the program does in a given environment and with a particular input. Therefore, one must collect env infos before performing such an analysis (ex: did it spread through malicious PDFs? In which OS was it detected?).

The idea of a sandbox is crucial. The idea is to run the malware inside an isolated and instrumented environment in order to observe the program's behavior. It makes the analysis immune to static obfuscation, anti-disas tricks, and packers. The sandbox must be able to:

* be easily reverted to a pristine state **(setup)**
* control and contain the malicious activities **(containment)**
* collect infos about the running sample **(instrumentation)**

### Chosing the right sandbox analysis method

**automated analysis / manual analysis**: automated is fast and the configuration is already supported by a team of experts, but manual analysis lets one tweak the inputs, and create an environment not known by attackers.

**real machine / VM**: real machines are accurate, undetectable, and not prone to evasion, but VMs are scalable and easily restorable/monitored. VM detection use the following techniques:

* VM-specific artefacts: device drivers name, many references to "VMWare" or similar, etc
* Look for differences in memory structures: <!-- TO FINISH -->

**running / emulating**: in a VM, either CPU+memory is emulated, or the whole computer is. Speed and flexiblity trade-off. Note that running a malware is legal but the conscequences (trojan spread, spam) might not. The traffic should be filtered: redirection of SMTP, emulation of well-known services...

Take into account to **operational security**: when analysis is performed, informations can be leaked. Beacons can be used to detect the presence of a manual analysis. Probes are special programs used to harvest data about the execcution environment and send back the infos.

**Triggers**: malware analysis usually spend a few minutes on each sample, detection can be avoided simply by waiting for a while. Logic bombs are samples that are triggered only given a specific event, like a set of keystrokes, a particular date, or the visit of a particular URL.

### Monitoring

Intercepting the functions called by a program help gaining an overview of the behavior of the program.

**Tracing** is a type of event logging. Linux' `strace` reports the system calls and the signals of a process while `ltrace` reports the library calls.

**DLL injection**: upon loading a DLL, the dllmain is executed in the address space of a process. One can force a process to load a DLL, thus arbitrary code can be run.

**Hooking** covers a range of technique intercepting and instrumenting function calls, messages or events before they pass from one component to another. Used for debugging, but also rootkits that fake API outputs so as to be undetected.

Techniques can be combined: CWSandbox injects a DLL that performs inline hooking of the API calls. All system objects that could reveal the presence of the analysis framework are sanitized by the hooks so that they never reach the process under analysis.

### Debugging

Debuggers are highly used. GDB Main options:

```
run [params]

stepi [n]
    executes n machine instructions

nexti [n]
    executes n function calls

continue

finish
    execute untill the stack frame returns

break <location>
break <location> if <expression>
    stop before the instruction at that location is executed

watch <expression>
rwatch <expression>
awatch <expression>
    set a watchpoint for the write, read, or access (r+w) of an expression

catch fork
catch syscall [which]
    Set a catchpoint (break on an certain event)

x/<n><u><f>
    n: how much units to display
    u: unit, in:
        b: bytes
        h: half-words
        w: words
        g: giant words
    f: display format, in:
        x: hex
        d: dec
        u: unsigned
        t: binary
        c: char
        f: float
        s: string
        i: machine instruction

info registers
    print the content of registers

set <expression>
return [value]
jump <location>
    alter the process state
```

Note that breakpoints usage in self-modifying code analysis is tricky because the breakpoint consists in replacing an instruction by another (INT3).

Anti-debugging tricks:

* INT3 detection
* Timing
* Only one debugger can be rattached to the program at the same time



## Computer forensics

### 101

Digital forensics involves the preservation, collection, validation, identification, analysis, interpretation, documentation, and presentation of digital evidences derived from computer media in a way that can be used in a court of law. History:

* 1978: first legislation against unauthorized data modification
* 1982: Norton's undelete program
* 1984: Fred Cohen coins the term virus
* 1984: FBI established the computer analysis and response team
* 1990's: international discussions about the legal frameworks for computer forensics
* 2004: convention on cybercrime (signed by 43 nations)

Framework of action:

1. Seizure (optional): marking and storgin all the components
2. Acquisition: acquire a copy without damaging the source
3. Analysis
4. Reporting

We'll only deal with the non-legal aspects of steps 1,2,3 here.

### Seizure

Pulling the plug is almost always better than pressing the power off button, which trigger processes and modify the memory. However, it might sometimes be easier to let the computer on:

* loss of volatile data
* loss of access to encrypted disk
* eg: facebook open, last messages readable

### Acquisition

Best practices:

* Go from volatile to less volatile
* Do not trust the acquisition programs on the target system

Distinction:

* Logical acquisition: `cp` and the likes. Only get what the system knows and want to show you (beware of rootkits and deleted files).
* Physical acquisition

Big issue: device might be physically affected.

### Analysis

Two approaches:

1. Looking for something we do not know (infected app, browser history, VoIP calls, running processes) in something we know (filesystem, PCAP, memory dump)
2. Looking for something we know (regex, signatures of files, keys, or URLs) in something we do not know (unknown acquired data structure)

### Indident response

Incidents are not-standards events that might cause service interruption/quality reduction. Includes:

* DoS
* Malware infection
* Modified web page
* Stolen credentials
* Deletion or modification of data

IR (incident response) goals:

1. Handle: identify, stop, contain the incident
2. Recover: remove, repair, restore the victim's system
3. Investigate: collect evidence, deduce the cause and the extent
4. Prevent



## Memory forensics

1. Acquisition of a memory snapshot
2. Locating known data structures
3. Carving de-allocated data structures

Available infos:

* rootkit
* open sockets, active connections
* clipboard
* caches
* processes

The memory acquisition problem: software-based methods are altering the system. Hardware-based methods are expensive and complex, plus the IOMMU sometimes prevent the access to the whole memory.

In cold boot attacks, the memory is physically frozen so as to be preserved. Boot from USB allows to manipulate the memory.

### Structures

Known kernel structures can help identify the data, but windows source code is not always available for documentation, and linux users can modify their kernel.

#### Processes

In windows, each process is identified by an executive process block (EPROCESS). All the EPROCESSes are connected in a double-linked list. Processes can be hidden by removing themselves from the list.

#### Memory

Memory analysis requires the ability to translate virtual addresses (used by programs) into physical ones (in the dump). Memory is divided into pages (@memory) or frames (@disk after a swap) of 4096 bytes each.

The virtual memory system stores such a mapping in the **page table** as **page table entries (PTEs)**.

#### Volatility (tool)

Memory analysis framework written in Python. Supports all the big OSs. Collection of tools implemented as plugins (python scripts that are installed simply by copying them in the plugins directory). 50 profiles, 265 plugins. Allows for:

* Image identification
* Process listing
* Lists the DLLs loaded by a process
* List open networks connections
* etc etc etc

#### Memory analysis

Often starts by investigating one process after another. Note that most malwares hides themselves by:

* removing themselves from process lists → do a process scan, not a process listing
* renaming themselves into system processes → do a dlllist to find where the process comes from
* inject their whole code into the dll of a legitimate process



## Network forensics

### 101

Several types of data:

* PCAP: lots of storage, but represents the whole traffic
* netflows: summarized flow data
* logs and alerts

That correspond to several tools:

* Sniffers: collects packets (eg dumpcap)
* Protocol analysers: packet-centric and/or session-centric capabilities (eg tcpdump, wireshark)
* network forensics analysis tools: data-centric analysis (eg NetworkMiner)

There also exists some specialised tools: mailsnarf to extract emails, ssl_dump to extract ssl infos, etc.

And some tools centered on programmable analysis, like libpcap, bro, or scapy.

Note that encrypted communication is the trend, so often the only things left to analyse are traffic volumes and timing.



### Data collection

#### Device

Taps are passive devices that duplicates all traffic and forward it to the monitoring port.

The monitoring device should make sure it sends no packets.

dumpcap is a libpcap-based tool for reading packets from a live network and saving them on disk. Uses the widely-used BPF syntax for packet filtering.

#### Format

The pcap format is the standard for packet captures. Format: `[main header] [[packet header] [packet]]*`. pcap-ng is its evolution, allowing for a support of multiple interfaces, comments, metadata, etc.

NetFlow summarizes IP traffic through sampling and summarization of packets. A flow is a sequence of packets that shares the same:

* IP src, IP dst
* port src, port dst
* IP protocol
* ingress interface

Note that a TCP connection is made up by 2 flows.



### Data analysis

capinfo is a tool for quickly getting infos on pcap files.

editcap is a useful tool for editing pcap files based on time windows truncation, packet index, etc.

wireshark is... well everyone knows what wireshark is. tshark is its command line interface - useful for tools combination.

Reverse DNS for resolving IP resolutions can be used, but a more reliable techiques is to track DNS queries.

Geolocation is useful but even if there exists many free tools on the net, there exists very few free IP geolocation databases or softwares.

OS fingerprinting is the process of determining which OS is running on a target system based on the network traffic it generates. Can also be used to detect multiple machines behind a NAT. OS respond differently to ICMP, TCP and DHCP, and have differet browser user agents. Can be easily fooled, although it is not likely to happen.

Several programmable libraries can be used for more control:

* libpcap - individual packets handle (C)
* pcapplusplus - individual packets handle (C++)
* scapy - individual packets handle (python)
* libnids - TCP sessions, emulates the IP stack (C)
* pynids - TCP sessions, emulates the IP stack (python)
* Bro - intrusion detection and complex realtime network analysis (custom language)

### Malwares and network

Three signs to look for:

* propagation
* malware behavior (eg: home probe report)
* malicious activity (eg: DoS)

#### Propagation

Drive-by download: infected website → redirect or load a malicious iframe or script. The landing page profiles the browser, choses the correct flaw to exploit, and installs the malware. Detection: signature of malicious iframes/scripts, blacklist of landing pages, malware detection.

Watering hole attack: company C orders pizzas at website W. Attacker A infects W so as to be able to do some further attack on C.

#### Botnets

A key component of a botnet is the C&C (command and control protocol). Not actually a protocol, but rather a framework of communication to enable the slave-master communication.

1. malicious server in plain sight (fail: arrest the malicious server)
2. malicious server in plain sight in a country with favorable laws (fail: firewall-out the server)
3. single flux: many IPs are registered then unregistered at a high frequence
4. double flux: single flux + rotates the authoritative server
5. domain flux: changing the domain name an infected machine connects to

#### Analysis

1. Check for known bad
    * blacklisted IPs or domains
    * extract all the transmitted file and malware-analyse them (fast method: MD5 on virustotal)
2. Check for anomalies
    * protocols on non-standards ports
    * IRC traffic
    * hardcoded IPs
    * spoofed and injected packets
3. Check the DNS traffic
    * DNS errors
    * anomalous TTL
    * dynamic DNS
4. Check the HTTP traffic
    * Redirects (too many of them)
    * User agents: sometimes still set to empty, perl or python
    * File extensions
    * HTTP on 443 (HTTPS rollback attach to HTTP)
    * Alexa top 1 million websites
5. Certificate
    * Self-signed
    * blacklisted

[PCAP dissector](https://github.com/eaam/Bro-PCAP-Dissector) automatize most of this.



## Disk forensics

`disk/drives -> volumes + partitions -> filesystem -> files`

### Disks and flahs drives

Actually several disks. Smallest addressable unit = **sector**. The physical address - sector number mapping is done by the disk controller. Disks can have hidden areas (not accessible to the OS), like the reserved service area.

Flash drives have the same interface as disks but work completely differently. Reading is easy but writing is hard because resetting transistors is hard, thus properly removing data from a flash drive is cumbersome.

During the acquisition phase, it is necessary to connect the disk to a write blocker.

Linux' `dd` command can help perform a simple logical copy of devices: `dd if=/dev/sda of=disk.raw`. Note that dd wass not designed with forensics in mind, but forks/patches exists that do.

`xmount` and similar tools allows users to boot from a disk image.

### Partitions and volumes

Disks are normally organized in **partitions**. **Volumes** are disk areas presented to the OS with a given file system. Under a letter in windows, managed by `mount` in Linux. Partitions can exist without a volume, volumes can exist without partitions, several volumes can be found under one partition.

### File systems

FS manipulates directories, files, and metadata (user-friendly). Often associated with an OS. Data is stored in allocation units called **blocks**. Files and free space gets block-fragmented.

FS forensics is useful because the FS doesn't cover the whole space available to the OS - and it can be used to hide data, or recover deleted files.

Deleted files:

* **deleted**: on disk: [metadata → data], but the file has been unlinked from the directory
* **orphaned**: on disk: [metadata, data], the metadata doesn't point to the data anymore
* **unallocated**: on disk: [data], content not yet overwriten
* **overwritten**: partial recovery can be made

Each block has an inode map (bitmap of free inodes in the block), unlinking a file comes down to setting to 0 the number of hard links in the file descriptor, and marking the area as free.

Moreover, using a magnetic force microscope, it's possible to read the previous bit value.

On SDD, "overwriting" a zone actually overwrites a zone somewhere else in memory, so it's pretty hard to actually overwrite data.

In NTFS, when deleting a file, its entry in the MFT is marked accordingly, and all of its clusters are marked as free. Neither the metada neither the cluster pointers get deleted.

Disk encryption: the encryption is only as secure as the key secretion, possible issues:

* cold boot attacks
* get the user reveal-his-key-to-strangers drunk
* keyloggers

Some people doing suspicious activities mind find handy that there exists tools that makes it impossible to even prove that some encrypted data exists on a disk.

### Malwares

Remember that only the firmware of the disk can access the reserved service area. Some people had the idea of infecting the firmware and hiding viruses in it.

**Bootkits** interfere with the boot process before the OS taks control, infecting the OS by loading malicious unsigned kernel modules.

Malware can hide their code in hidden files.



## Techniques

### Entropy

Based on entropy, the file type can generally be predicted:

| entropy | prediction |
| --- | --- |
| >7 bits per byte | encrypted data |
| ~5 bits per byte | programming language |
| ~3 bits per byte | machine code |
| <2 bits per byte | uncompressed data |

### Hashes

Used for identification of known bad files and filtering of known good files. But:

* Blind to harmless small modifications
* Blind to partial comparison
* Possible fragmentation
* The file can be stored in different ways, eg on disk vs on memory

Possible countermeasures:

* block-based hashes
* random sampling (if the data we're looking for is big enough, the probability of not finding it gets low quick)
* other tools for similarity measurements:
    * ssdeep: two files are similar if they share sub-parts
    * sdhash: two files are similar if they share improbable sequences of bytes
    * tlsh: two files are similar if they have similar distributions of substrings of n bytes

### Characters encoding

* ASCII
* UNICODE (utf-8 and utf-16, beware of the endianness for the last one)
* %-encoding
* hexadecimal: 2bytes per char: A->41
* UUencoding: encode 6 bits at a time + footer/header
* base64: 6 bits at a time, MIME supported
* quoted-printable: like %-encoding, but with a = instead. Escapes the non-readable chars

### Carving

Carving files gets efficient by carving its heading and footer, but ther might be a lot of false positives if they are too short. Another approach is to base the search on the structure, or on the content: the entropy or the byte distribution might reveal the file type or other informations.

Blocks with high entropy are likely unique. The existence of such a block (hash-identified) on a disk is likely to reveal the presence of the file the unique block comes from.

Note that carving can also extract useless informations: for example there exists 20K email addresses on a *clean* fedora distribution. Tools like `bulk_extractor` support list of known features to filter out.



## OS and application forensics

### OS-independent

#### Files

File type determine the structure of the data but it is stored nowhere: windows encode it in the extension, but it can be changed; Linux considers everything as a text file but infos can be stored in the header or footer, but it can be fooled too. The only thing left is to look at the content - still a forensics research topic.

Some format don't care if data is appened at the end of their file: that's the case of jpg, zip, elf and mp3. The first file can be used normally and a file can be invisibly appened.

#### Metadata

Often more important that the data itself.

`extract` is a general tool for getting metadata but ad-hoc specific tools perform better: `exiv2` for images, `pdfinfo` for pdf...

Time analysis is useful to understand what event happened. Super-timelines are object that contain even more infos than the filesystem can provide. They have to go through data reduction and anomalies detection processes.

#### Web browsers

Chromes uses sqlite3 databases to store most infos (TL;DR sqlite3 allows for databases in a single file). Deleted rows remain in the file untill they are overwritten... Cookies also might store further infos that can't be found on disk.

Chrome is the most used browser (50% and growing, Firefox and Safari are both below 15%).



### Linux

Most logs are in plaintext so it's cool.

Often installed on ext4 filesystem, so it may be hard to recover deleted files.

/etc contains the configuration files of applications (plain text). Some are predictable:

* computer name: `/etc/hostname`
* release: `/etc/*-release`
* account infos: `/etc/passwd`
* passwords and hashes: `/etc/shadow`

`~/.ssh/config` and `~/.ssh/known_hosts` contain ssh infos like the list of hashes of hosts the user connected to. Shell history in `~/.bash_history` (caution: can be freely edited).

There's a lot of infos in the logs, which have only a definite structure for the time, hostname and process.



### Windows

Windows **registry** is a hierarchical DB that contains infos about users, settings, apps, devices, and events. Organised in **hives**, that are themselves organised in **cells**. Fred is a registry hive viewer. Rip is a perl program that can also perform the fetch of useful infos like:

* Malwares heavily rely on automatic launch upon boot. The **autostart locations** can help identify them.
* Footprints of external device connections.
* In the NTUSER hive, all the applications used by a user are logged with a counter. ROT13 encryption!

LNK exists for opened files and help do stuff like build the "recently opened" view. Shellbags are used to store the user's preferences to display directories, created upon directory navigation.

Binary log of events, more or less the same system as in windows.